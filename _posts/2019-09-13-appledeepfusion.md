---
layout: post
---

## Apple's Deep Fusion Technology

While Apple usually explains complicated technologies in a simple to understand manner -- arguably the best in the industry -- honestly, the 10 September event was confusing, light on details, and quite underwhelming. Exciting technologies such as the U1 chip, Deep Fusion, and SoC details [for Apple Watch Series 5] barely got any mention. There was way too much time dedicated to game demos, advertising videos, and speaker transitions. Not to mention a very head-tilting naming scheme for the new lineup of iPhones. *iPhone 11 Pro Max*... really?

Why not simply name the lineup: *iPhone 11* and *iPhone 11 Pro*, and then differentiate the two Pro iPhones by size? This would at least bring some consistency between *iPhone Pro*, *MacBook Pro*, and *iPad Pro*. Heck, I've been hoping Apple would drop model numbers entirely and just refer to iPhones by their launch year. I think most people would agree that *iPhone Pro* (2019, 6.5-inch) is a lot better than *iPhone 11 Pro Max*. But anyway, my issues with Apple's confusing naming scheme should be saved for another time.

During this week's keynote, Phil Schiller took stage to explain some of the secrets behind the iPhone 11's camera technology. Of particular note and interest was Deep Fusion. Schiller's explanation left a lot to be desired -- and a lot to the imagination. Frustratingly, a lot of tech journalists have little idea what Deep Fusion is and have struggled to explain what the technology does. For example, this [engadget article](https://www.engadget.com/2019/09/10/apple-iphone-deep-fusion/) says that Deep Fusion is Apple's technology to tackle Google's Night Sight. This is completely false. Apple will be employing Night Mode to do so.

My understanding of Deep Fusion is that it is a machine learning (ML) based technology, powered by the A13 Bionic's Neural Engine to replace or sidestep the camera's image signalling processor (ISP) image capturing pipeline. My guess is that Deep Fusion only replaces the ISP's duties for still photos, and that the ISP does most of the heavy lifting for other photo modes and video. When taking a photo, the A13 Bionic and iPhone's camera lenses will basically take nine images: four short burst images taken before the user hits the photo button, four short burst images once the user hits the photo button, and a single long exposure image.

The A13 Bionic's Neural Engine will then analyse all nine images, applying various algorithms at the pixel and sub-pixel level to blend the images and combine them into a final image. The closest comparison I can think would be advanced anti-aliasing filtering techniques from the likes of AMD and Nvidia when it comes to 3D rendering pipelines. The key point is that the A13 Bionic determines how the final image is put together, pixel by pixel, rather than the camera ISP bluntly and broadly fusing multiple captured images. The technology is remarkable, and I'm just so surprised that Apple didn't take more time to explain it further.

I'm sure someone from the likes of Anandtech or Arstechnica will explore Deep Fusion soon.
